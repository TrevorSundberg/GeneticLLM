
Write this like a genetic algorithm
 - generate all the sources
 - each sets of sources that get closer to the answer

we let each AI go a number of "iterations" to step toward the goal
 - we measure fitness first by correct compilations, and inverse length of compile errors
 - then by how close we get to matching the output of each test (can just use inverse length of diff reports)
 - then by number of tests passed
 - tiered system

run many variations, or "children"
 - potentailly even allow seed attempts by the user

run them with an initial seed, get a fitness score

but we're kind of saying that the code is the gene, so the thing develops it's own genes...

but also that kind of is the seed, the gene is the solution

so every one of them starts of generating a solution, just initially
 - different seed, no randomness

aight, gonna run this on a super-comp

it's a genetic framework with AI results

mutation can be like find a token and replace it,
or a single character, or ask the llm to mutate... lots of options


when we write asserts, we should do it by running it over all test outputs from the original sources
 - the asserts should pass every test
 - then the cool thing is we can run the asserts against the outputs

for scoring, we should take the best iteration (like score each pass that it makes, take the highest)
 - basically how many iterations is how many times we're allowing the LLM to give feedback

for C, can compile with as many linters as we want
 - just to give better feedback to the AI, not allow compilation with any linting errors
 - also pass ansi-pedantic -Wall -Wextra style



we translate file by file, expecting files to not outdo the context size
 - in a robust version, we cap the input and output size so we can't exceeed context
 - may want to be able to recompile the source too
   - as we might want to make modifications to break it up and make it easier to translate

since we're going file by file, when errors are reported, they will have the file names (ideally)
 - the AI can respond to the errors and file names

the initial AI makes a translation pass for every compilation unit (file)

we have an agent that allows the AI to pick the file to make edits to


scoring, 0/1 for compiling

if it's not compiling, then the score is negative by the length of the compiler errors
 - not a perfect metric, but it's an approximation
 - can do something later that counts the number of errors
   - but that's not even perfect, it's possible to have 100 errors but a one letter change to fix it
   - but also have 10 errors, and each require their own fix 
 - so ultimately it's just an approximation

for positive scores, +1 for every unit test passed
 - or, really + [0,1], where the value is how close the output is to the expected output


inputs are:
 - population size
 - llm iterations
 - mutation rate
 - 

alternatively, if we want a fitness that converges to 0

if it doesn't compile:
finess: -(test_count + error_count)

if it compiles:
fintess: failing_tests

if it passes all tests:
measure performance

>= 0 fitness means we have a correct solution

side note, fitness doesn't need to be boiled down to a single number
 - as long as it's a comparable key

GeneticLLM is a pretty cool name actually
genetic_llm

genetic_llama


-------------
side note, at the end of each iteration, the genetic code can
have extra parameters that allow it to generate new unit tests
 - we can base initial extras off the initial code
 - but it might be beneficial to get more off generated target code
 - again, we're just asking the LLM for all this anyways

we can even do some de-duping to try and reduce inputs that all have the same output (error, etc)

genetic prompt engineering

genetic code translation


-------------------------

this is going to be my proof

if I can take a reasonably complex code and wake up in the morning and see it translated...

then I will buy the superpower machine, it's worth it
I can just start translating things

GeneticLLM
 - code translation
 - prompt engineering

how do we do the prompt engineering?
 - we have to define a goal
 - a set of test cases, similar to code
 - maybe even a grammar

it's similar to the compiler approach, but we don't have compiler errors or performance... or maybe we do have performance?
 - I'm actually not sure, do LLMs take longer with different prompts?

--------------

how to improve genetic algorithm results:
for one, the 50% mixing doesn't work super well (sometimes it just picks an entire one)
 - also, can't just mix "genes"
   - I feel like in real life, you can pick random genes from both
   - and somehow end up with a still good "solution", or viable living animal
   - but here, all the schemes I can think of for picking "genes" all create a 99.999% change the being would be dead
   - basically just code that 

I think what might be better for recombining is just doing something line by line
 - and then ask the llm to fix it up

mutation also seems silly, but then again that's part of the llm fixup

maybe we just need better prompts, how can we guide the llm


what we need is to assist the LLM in generating the best code from the start
 - 

possibly find other better LLMs for this task too
 - using a quite old one at this point


I think what we need to do is allow an LLM to loop many times
 - to fix the code
 - using a bunch of different test seeds
 - or maybe even write this into a genetic algorithm
 - basically just 

but it's not just the prompt itself, it's also like, how concatenate all the info together
 - almost like what we want is to make simple replacements
 - but really, it's just simple code

almost want to generate code to use for prompts


also should reduce timing
 - or measure max timing from each test (run multiple times for timing)
 - then make it a multiple of the max (that's the timeout we use), like 10x
 - ideally it ends up being really small, less than 5 seconds for our current tests


lots more tests possibly?
 - also don't have redundant tests
 - like 1, 2, 3, 4, 5 and 5, 4, 3, 2, 1
   - where the output is the same


given a json object, generate a prompt in pure JS (sandboxed basically)

then when we run that code, repeatedly

maybe when you run the command, we get the active terminal


I think one primary problem is that it's not easy for the AI to "hill climb"
 - not enough tests

hiding the tests (or having a layer of indirection)
 - this will help to not just target a single test


first, without seeing any code
 - give the LLM the erroneous output and the expected output, and ask it what needs to happen to the output to fix it
 - then do the same thing, but provide the code that generated the response, and ask how to fix that code
 - after that, put both explanations along with the original code and last translation, and ask it to fix the issues


should we penalize programs that crash / timeout?
 - I mean, the output will be wrong anyways



no ambiguous tests

diff comparison of output - with some kind of feedback on 

In a way, I think similiarty can be reated a few ways:
 - allow LLM to rate it, I like this because it can do a little more reasoning (like both are lists of numbers)
 - sentence similarity - a bit more predictable than an LLM, which sometimes can hallucinate
 - length matching (just purely the length of the string)
 - diff (will output nothing if it diffs)
 - also should just do an exact direct compare (strcmp) first, since LLM might go unhinged and say two exact things aren't equal

so we go through all the tests and sum it up (1 - result)
 - since we measure failed tests
 - maybe can change that from failed tests into test similarity (higher values, add all together)
   - can also divide by number of tests to get a normalized number, just looks nice
-------------------

As a perfect example, I asked the most advanced version of GPT (4o currently) to do the translation
the result was a fully compiling program, however the output was sometimes wrong

this is amazing because it means if we improve the LLM (bigger model, more advanced model, bigger gpu, etc)
 - then we should just get better results

ok, lets get something we can run by tonight

DONE - add length calculations to the scoring too for the inputs
DONE - more tests, non-ambiguous results

side note, we could tokenize the test results and specifically penalize any mention of them
 - even in normal prompting, not just in the reasoning
 - that would guide the generated code away from directly hardcoding outputs...

things to investigate:
 - crossover (doesn't seem to be working well)
 - mutation (also kinda eh)
 - better LLM output with reasoning (given the original code, its translation, and the test error, ask it how to fix)
 - we should maybe just have a limit to how many errors we show from the tests
   - encapsulate all these settings, temperature, etc, in an object that we pass along with the model
   - LLM configuration, sub config of the genetic thing
   - it's basically an object that we ask everything to for building the prompts (just pass JSON, like all the failed tests)
   - allow them to take their own options and configure it however they want

I think we should categorize programs that crash
 - and then programs that have all the same output regardless of tests
 - if every test outputs the same thing (crash, etc)
 - as long as it's not expected by the sample program

basically just combining outputs
 - any time we encounter an output that's the same, we combine it
 - what if the sample was the same, what if it was different?
 - we also at some point kind of want to show the input too,
   to make sure it understands that it needs to target multiple inptus

we want to penalize crashing and penalize outputs that are entirely the same (no variation, unless that's expected)
 - maybe what we do is compare the sample outputs together to see similarity (e.g. remove duplicates, deduped/total)
 - and if our result is lower than that, we're penalized
 - that naturally penalizes crashes, timeouts, etc! I kinda like it!
 - We should also gather all the sample outptus ahead of time since we're doing this change

maybe we can measure diversity by the test outputs?
 - would possibly need to be part of MeasuredCandidate (the test outputs)
   - since we compare diversity N^2 between all candidates
   - but only when breeding is it really needed

I also want to run the LLM after mutation because when I think about it,
it's a bit of an unfair advantage that the graduated candidates have, or even mutated candidates (since they ran the LLM)

------------------

What's the idea with the "all the same" thing?
 - I think maybe instead of having a special case where it's all the same, I should maybe group it...?
 - like what more can I do... ultimately the test feedback knows it crashed
 - I think we should first do the refactor where we allow the user to specify the model + prompts
 - we just feed json data to them, and let them do the prompting themselves (give them the llm instance / session)

let the user build all the prompts, but provide a default

then we can do whatever kind of grouping we want, play with all that
 - and it's nice and sandboxed in one area, not complicated / mixed with the error/fitness stuff

fixCompileErrors
fixTestErrors
rateSimilarity
crossover
mutate

also allow them to specify the sentence compare model, or possibly make that a callback (but provide an implementation)

I think this is also the refactor we'd want to do first before attempting the "give feedback on how to fix this"

One thought is that we observed a behavior we didn't like, and then wrote something to help guide it away from that behavior
 - I can see in the future where users may want to write custom code to guide situations
 - or even have human intervention
 - it also ideally helps it to accelerate toward the goal

------------------

Competitor research:

https://developerpal.dev/translate
 - basically just an LLM, nothing special, code doesn't work, probably just GPT (compiled, but did not pass all tests)
https://www.codeconvert.ai/free-converter
 - same, compiled with warnings, failed tests
https://translatte.ai/
 - can't use it without connecting github, but pretty much looks the same
https://aicodeconvert.com/
 - same...
https://dev.to/github/how-to-translate-code-into-other-languages-using-github-copilot-3n6f
 - just a blog post, but basically dissapointing results from a comment

https://straico.com/top-10-ai-code-translators
 - I feel like this article was generated by AI lol

------------------
TODO:

Separate out prompting config / string generation
Test LLM feedback on code idea (passing in test inputs and outputs)
Automatically measure expected output lengths (and boost them by factor?)
Automatically measure expected running times (and boost them by factor?)
Measure diversity using fitness results (possibly also compare code, sentence likeness...)
Islanding with separate populations and a method for migration between them

---------
easier setup script to use on vast.... it's like... not any faster than my machine...


