
Write this like a genetic algorithm
 - generate all the sources
 - each sets of sources that get closer to the answer

we let each AI go a number of "iterations" to step toward the goal
 - we measure fitness first by correct compilations, and inverse length of compile errors
 - then by how close we get to matching the output of each test (can just use inverse length of diff reports)
 - then by number of tests passed
 - tiered system

run many variations, or "children"
 - potentailly even allow seed attempts by the user

run them with an initial seed, get a fitness score

but we're kind of saying that the code is the gene, so the thing develops it's own genes...

but also that kind of is the seed, the gene is the solution

so every one of them starts of generating a solution, just initially
 - different seed, no randomness

aight, gonna run this on a super-comp

it's a genetic framework with AI results

mutation can be like find a token and replace it,
or a single character, or ask the llm to mutate... lots of options


when we write asserts, we should do it by running it over all test outputs from the original sources
 - the asserts should pass every test
 - then the cool thing is we can run the asserts against the outputs

for scoring, we should take the best iteration (like score each pass that it makes, take the highest)
 - basically how many iterations is how many times we're allowing the LLM to give feedback

for C, can compile with as many linters as we want
 - just to give better feedback to the AI, not allow compilation with any linting errors
 - also pass ansi-pedantic -Wall -Wextra style



we translate file by file, expecting files to not outdo the context size
 - in a robust version, we cap the input and output size so we can't exceeed context
 - may want to be able to recompile the source too
   - as we might want to make modifications to break it up and make it easier to translate

since we're going file by file, when errors are reported, they will have the file names (ideally)
 - the AI can respond to the errors and file names

the initial AI makes a translation pass for every compilation unit (file)

we have an agent that allows the AI to pick the file to make edits to


scoring, 0/1 for compiling

if it's not compiling, then the score is negative by the length of the compiler errors
 - not a perfect metric, but it's an approximation
 - can do something later that counts the number of errors
   - but that's not even perfect, it's possible to have 100 errors but a one letter change to fix it
   - but also have 10 errors, and each require their own fix 
 - so ultimately it's just an approximation

for positive scores, +1 for every unit test passed
 - or, really + [0,1], where the value is how close the output is to the expected output


inputs are:
 - population size
 - llm iterations
 - mutation rate
 - 

alternatively, if we want a fitness that converges to 0

if it doesn't compile:
finess: -(test_count + error_count)

if it compiles:
fintess: failing_tests

if it passes all tests:
measure performance

>= 0 fitness means we have a correct solution

side note, fitness doesn't need to be boiled down to a single number
 - as long as it's a comparable key

GeneticLLM is a pretty cool name actually
genetic_llm

genetic_llama


-------------
side note, at the end of each iteration, the genetic code can
have extra parameters that allow it to generate new unit tests
 - we can base initial extras off the initial code
 - but it might be beneficial to get more off generated target code
 - again, we're just asking the LLM for all this anyways

we can even do some de-duping to try and reduce inputs that all have the same output (error, etc)

genetic prompt engineering

genetic code translation


-------------------------

this is going to be my proof

if I can take a reasonably complex code and wake up in the morning and see it translated...

then I will buy the superpower machine, it's worth it
I can just start translating things

GeneticLLM
 - code translation
 - prompt engineering

how do we do the prompt engineering?
 - we have to define a goal
 - a set of test cases, similar to code
 - maybe even a grammar

it's similar to the compiler approach, but we don't have compiler errors or performance... or maybe we do have performance?
 - I'm actually not sure, do LLMs take longer with different prompts?

--------------

how to improve genetic algorithm results:
for one, the 50% mixing doesn't work super well (sometimes it just picks an entire one)
 - also, can't just mix "genes"
   - I feel like in real life, you can pick random genes from both
   - and somehow end up with a still good "solution", or viable living animal
   - but here, all the schemes I can think of for picking "genes" all create a 99.999% change the being would be dead
   - basically just code that 

I think what might be better for recombining is just doing something line by line
 - and then ask the llm to fix it up

mutation also seems silly, but then again that's part of the llm fixup

maybe we just need better prompts, how can we guide the llm


what we need is to assist the LLM in generating the best code from the start
 - 

possibly find other better LLMs for this task too
 - using a quite old one at this point


I think what we need to do is allow an LLM to loop many times
 - to fix the code
 - using a bunch of different test seeds
 - or maybe even write this into a genetic algorithm
 - basically just 

but it's not just the prompt itself, it's also like, how concatenate all the info together
 - almost like what we want is to make simple replacements
 - but really, it's just simple code

almost want to generate code to use for prompts


also should reduce timing
 - or measure max timing from each test (run multiple times for timing)
 - then make it a multiple of the max (that's the timeout we use), like 10x
 - ideally it ends up being really small, less than 5 seconds for our current tests


lots more tests possibly?
 - also don't have redundant tests
 - like 1, 2, 3, 4, 5 and 5, 4, 3, 2, 1
   - where the output is the same


given a json object, generate a prompt in pure JS (sandboxed basically)

then when we run that code, repeatedly

maybe when you run the command, we get the active terminal


I think one primary problem is that it's not easy for the AI to "hill climb"
 - not enough tests

hiding the tests (or having a layer of indirection)
 - this will help to not just target a single test


first, without seeing any code
 - give the LLM the erroneous output and the expected output, and ask it what needs to happen to the output to fix it
 - then do the same thing, but provide the code that generated the response, and ask how to fix that code
 - after that, put both explanations along with the original code and last translation, and ask it to fix the issues


should we penalize programs that crash / timeout?
 - I mean, the output will be wrong anyways



no ambiguous tests

diff comparison of output - with some kind of feedback on 

In a way, I think similiarty can be reated a few ways:
 - allow LLM to rate it, I like this because it can do a little more reasoning (like both are lists of numbers)
 - sentence similarity - a bit more predictable than an LLM, which sometimes can hallucinate
 - length matching (just purely the length of the string)
 - diff (will output nothing if it diffs)
 - also should just do an exact direct compare (strcmp) first, since LLM might go unhinged and say two exact things aren't equal

so we go through all the tests and sum it up (1 - result)
 - since we measure failed tests
 - maybe can change that from failed tests into test similarity (higher values, add all together)
   - can also divide by number of tests to get a normalized number, just looks nice
-------------------

As a perfect example, I asked the most advanced version of GPT (4o currently) to do the translation
the result was a fully compiling program, however the output was sometimes wrong

this is amazing because it means if we improve the LLM (bigger model, more advanced model, bigger gpu, etc)
 - then we should just get better results


ok, lets get something we can run by tonight




